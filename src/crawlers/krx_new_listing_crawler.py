#!/usr/bin/env python3
"""
KRX ì‹ ê·œìƒì¥ ì¢…ëª© í¬ë¡¤ëŸ¬
https://kind.krx.co.kr/listinvstg/listingcompany.do?method=searchListingTypeMain
"""

import requests
import polars as pl
from bs4 import BeautifulSoup
import logging
import time
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class KRXNewListingCrawler:
    """KRX ì‹ ê·œìƒì¥ ì¢…ëª© í¬ë¡¤ëŸ¬"""

    def __init__(self, data_dir: str = "data/raw"):
        self.base_url = "https://kind.krx.co.kr/listinvstg/listingcompany.do"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # ì„¸ì…˜ ìƒì„±
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

        # ì‹œì¥ êµ¬ë¶„ ì½”ë“œ ë§¤í•‘
        self.market_codes = {
            'ALL': '',           # ì „ì²´
            'KOSPI': 'stockMkt', # ìœ ê°€ì¦ê¶Œì‹œì¥
            'KOSDAQ': 'kosdaqMkt', # ì½”ìŠ¤ë‹¥ì‹œì¥
            'KONEX': 'konexMkt'  # ì½”ë„¥ìŠ¤ì‹œì¥
        }

        # ìƒì¥ ìœ í˜• ì½”ë“œ
        self.listing_types = {
            'ALL': '',              # ì „ì²´
            'NEW': 'ì‹ ê·œìƒì¥',       # ì‹ ê·œìƒì¥
            'TRANSFER': 'ì´ì „ìƒì¥',  # ì´ì „ìƒì¥
            'RELIST': 'ì¬ìƒì¥'       # ì¬ìƒì¥
        }

    def _get_search_form_data(self, market_code: str, listing_type: str = 'NEW',
                             start_date: str = None, end_date: str = None) -> Dict[str, str]:
        """ê²€ìƒ‰ í¼ ë°ì´í„° ìƒì„±"""
        # ê¸°ë³¸ê°’: ìµœê·¼ 30ì¼
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=30)).strftime("%Y%m%d")
        if end_date is None:
            end_date = datetime.now().strftime("%Y%m%d")

        form_data = {
            'method': 'searchListingTypeList',
            'currentPageSize': '5000',  # í° í˜ì´ì§€ ì‚¬ì´ì¦ˆ
            'pageIndex': '1',
            'orderMode': '0',
            'orderStat': 'D',
            'fromDate': start_date,
            'toDate': end_date,
            'companyNm': '',  # íšŒì‚¬ëª… ë¯¸ì§€ì •
            'nationality': '',  # êµ­ì  ì „ì²´
            'industry': '',  # ì—…ì¢… ì „ì²´
            'listingAdvsr': '',  # ìƒì¥ì£¼ì„ ì‚¬ ì „ì²´
            'secuKind': '',  # ì¦ê¶Œì¢…ë¥˜ ì „ì²´
        }

        # ì‹œì¥ ì„ íƒ
        if market_code:
            form_data[market_code] = 'on'

        # ìƒì¥ìœ í˜• ì„ íƒ (ì‹ ê·œìƒì¥ì´ ê¸°ë³¸)
        if listing_type == 'NEW':
            form_data['listingType1'] = 'on'  # ì‹ ê·œìƒì¥
        elif listing_type == 'TRANSFER':
            form_data['listingType2'] = 'on'  # ì´ì „ìƒì¥
        elif listing_type == 'RELIST':
            form_data['listingType3'] = 'on'  # ì¬ìƒì¥
        else:
            # ì „ì²´ ì„ íƒ
            form_data['listingType1'] = 'on'
            form_data['listingType2'] = 'on'
            form_data['listingType3'] = 'on'

        return form_data

    def _download_excel_data(self, market_code: str, market_name: str,
                           listing_type: str = 'NEW', start_date: str = None,
                           end_date: str = None) -> Optional[Path]:
        """Excel í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        try:
            # 1. ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            logger.info(f"Accessing main page for {market_name} new listings...")
            main_response = self.session.get(f"{self.base_url}?method=searchListingTypeMain")
            main_response.raise_for_status()
            time.sleep(1)

            # 2. ê²€ìƒ‰ í¼ ë°ì´í„° ì¤€ë¹„
            form_data = self._get_search_form_data(market_code, listing_type, start_date, end_date)

            # 3. ê²€ìƒ‰ ì‹¤í–‰
            logger.info(f"Searching {listing_type} listings for {market_name}...")
            search_response = self.session.post(self.base_url, data=form_data)
            search_response.raise_for_status()
            time.sleep(2)

            # 4. Excel ë‹¤ìš´ë¡œë“œ
            excel_data = form_data.copy()
            excel_data['method'] = 'searchListingTypeExcel'

            logger.info(f"Downloading Excel data for {market_name}...")
            excel_response = self.session.post(self.base_url, data=excel_data)
            excel_response.raise_for_status()

            # 5. íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{market_name.lower()}_new_listing_{listing_type.lower()}_{timestamp}.html"
            file_path = self.data_dir / filename

            # ì¸ì½”ë”© ì²˜ë¦¬
            content = ""
            try:
                content = excel_response.content.decode('euc-kr')
            except UnicodeDecodeError:
                try:
                    content = excel_response.content.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        content = excel_response.content.decode('cp949')
                    except UnicodeDecodeError:
                        content = excel_response.content.decode('latin1')

            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)

            logger.info(f"âœ… {market_name} new listing data downloaded: {file_path} ({len(content):,} characters)")

            # ë‚´ìš© í™•ì¸
            if "ì˜¤ë¥˜" in content or "error" in content.lower() or len(content) < 1000:
                logger.warning(f"Downloaded content might be an error page for {market_name}")

            return file_path

        except Exception as e:
            logger.error(f"âŒ Failed to download {market_name} new listing data: {e}")
            return None

    def parse_html_to_dataframe(self, html_file: Path, market_name: str) -> pl.DataFrame:
        """HTML íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜"""
        try:
            logger.info(f"Parsing HTML file: {html_file}")

            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()

            soup = BeautifulSoup(html_content, 'html.parser')

            # í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            if not tables:
                logger.warning(f"No tables found in {html_file}")
                return pl.DataFrame()

            # ê°€ì¥ í° í…Œì´ë¸”ì„ ë°ì´í„° í…Œì´ë¸”ë¡œ ê°€ì •
            main_table = max(tables, key=lambda t: len(t.find_all('tr')))
            rows = main_table.find_all('tr')

            if len(rows) < 2:
                logger.warning(f"Not enough rows in table: {len(rows)}")
                return pl.DataFrame()

            # í—¤ë” ì¶”ì¶œ
            header_row = rows[0]
            headers = [th.get_text().strip() for th in header_row.find_all(['th', 'td'])]
            logger.info(f"Found headers: {headers}")

            # ë°ì´í„° ì¶”ì¶œ
            data_rows = []
            for row in rows[1:]:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= len(headers):
                    row_data = [cell.get_text().strip() for cell in cells[:len(headers)]]
                    data_rows.append(row_data)

            if not data_rows:
                logger.warning(f"No data rows found in {html_file}")
                return pl.DataFrame()

            # DataFrame ìƒì„±
            df_data = {header: [] for header in headers}
            for row in data_rows:
                for i, header in enumerate(headers):
                    df_data[header].append(row[i] if i < len(row) else '')

            df = pl.DataFrame(df_data)

            # ì»¬ëŸ¼ëª… ì •ê·œí™”
            df = self._normalize_columns(df, market_name)

            logger.info(f"âœ… Parsed {len(df)} records from {html_file}")
            return df

        except Exception as e:
            logger.error(f"âŒ Failed to parse {html_file}: {e}")
            return pl.DataFrame()

    def _normalize_columns(self, df: pl.DataFrame, market_name: str) -> pl.DataFrame:
        """ì»¬ëŸ¼ëª… ì •ê·œí™” ë° ë°ì´í„° íƒ€ì… ë³€í™˜"""
        try:
            # ì‹ ê·œìƒì¥ ë°ì´í„° ì»¬ëŸ¼ëª… ë§¤í•‘
            column_mapping = {
                'íšŒì‚¬ëª…': 'company_name',
                'ì¢…ëª©ì½”ë“œ': 'company_code',
                'ìƒì¥ì¼': 'listing_date',
                'ì‹œì¥êµ¬ë¶„': 'market_type',
                'ìƒì¥ìœ í˜•': 'listing_type',
                'ì—…ì¢…': 'industry',
                'ì•¡ë©´ê°€': 'par_value',
                'ê³µëª¨ê°€ê²©': 'ipo_price',
                'ê³µëª¨ê¸ˆì•¡': 'ipo_amount',
                'ì£¼ìš”ì œí’ˆ': 'main_products',
                'ìƒì¥ì£¼ì‹ìˆ˜': 'listed_shares',
                'ìƒì¥ì£¼ì„ ì‚¬': 'listing_advisor',
                'êµ­ì ': 'nationality',
                'ìˆœë²ˆ': 'sequence'
            }

            # ì»¬ëŸ¼ëª… ë³€ê²½
            current_columns = df.columns
            rename_dict = {}
            for old_col in current_columns:
                for kr_name, en_name in column_mapping.items():
                    if kr_name in old_col:
                        rename_dict[old_col] = en_name
                        break

            if rename_dict:
                df = df.rename(rename_dict)
                logger.info(f"Renamed columns: {rename_dict}")

            # ì‹œì¥ ì •ë³´ ì¶”ê°€
            df = df.with_columns(pl.lit(market_name).alias('market'))

            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ (ìƒì¥ì¼)
            if 'listing_date' in df.columns:
                df = df.with_columns(
                    pl.col('listing_date')
                    .str.replace_all(r'[^\d]', '')  # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì œê±°
                    .str.strptime(pl.Date, format='%Y%m%d', strict=False)
                    .alias('listing_date')
                )

            # ì¢…ëª©ì½”ë“œ ì •ë¦¬ (6ìë¦¬ ìˆ«ìë§Œ)
            if 'company_code' in df.columns:
                df = df.with_columns(
                    pl.col('company_code')
                    .str.replace_all(r'[^\d]', '')  # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì œê±°
                    .str.slice(0, 6)  # ì²˜ìŒ 6ìë¦¬ë§Œ
                    .alias('company_code')
                )

                # 6ìë¦¬ê°€ ì•„ë‹Œ ì¢…ëª©ì½”ë“œ í•„í„°ë§
                df = df.filter(pl.col('company_code').str.len_chars() == 6)

            # ìˆ«ì ì»¬ëŸ¼ ì²˜ë¦¬
            numeric_columns = ['par_value', 'ipo_price', 'ipo_amount', 'listed_shares']
            for col in numeric_columns:
                if col in df.columns:
                    df = df.with_columns(
                        pl.col(col)
                        .str.replace_all(r'[^\d.]', '')  # ìˆ«ìì™€ ì†Œìˆ˜ì ì´ ì•„ë‹Œ ë¬¸ì ì œê±°
                        .cast(pl.Float64, strict=False)
                        .alias(col)
                    )

            # ë¹ˆ ë¬¸ìì—´ì„ nullë¡œ ë³€í™˜
            for col in df.columns:
                if df[col].dtype == pl.String:
                    df = df.with_columns(
                        pl.col(col).replace('', None)
                    )

            return df

        except Exception as e:
            logger.error(f"Failed to normalize columns: {e}")
            return df

    def crawl_market(self, market_name: str, listing_type: str = 'NEW',
                    start_date: str = None, end_date: str = None) -> Optional[pl.DataFrame]:
        """íŠ¹ì • ì‹œì¥ì˜ ì‹ ê·œìƒì¥ ì¢…ëª© í¬ë¡¤ë§"""
        if market_name not in self.market_codes:
            logger.error(f"Invalid market name: {market_name}. Valid options: {list(self.market_codes.keys())}")
            return None

        market_code = self.market_codes[market_name]

        logger.info(f"ğŸš€ Starting new listing crawl for {market_name} market (type: {listing_type})")

        # 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        html_file = self._download_excel_data(market_code, market_name, listing_type, start_date, end_date)
        if not html_file:
            return None

        # ì ì‹œ ëŒ€ê¸°
        time.sleep(2)

        # 2. HTML íŒŒì‹±
        df = self.parse_html_to_dataframe(html_file, market_name)

        return df

    def crawl_all_listings_full_sync(self, start_year: int = 2000, listing_type: str = 'NEW') -> pl.DataFrame:
        """ì „ì²´ ìƒì¥ ë°ì´í„° ë™ê¸°í™” (ì¼ê°„ ë°°ì¹˜ìš©)"""
        start_date = f"{start_year}0101"
        end_date = datetime.now().strftime("%Y%m%d")

        logger.info(f"ğŸš€ Full sync crawling all listings ({start_date} ~ {end_date})")

        all_dfs = []

        # KOSPI, KOSDAQ, KONEX ê°ê° í¬ë¡¤ë§
        for market_name in ['KOSPI', 'KOSDAQ', 'KONEX']:
            try:
                df = self.crawl_market(market_name, listing_type, start_date, end_date)
                if df is not None and not df.is_empty():
                    all_dfs.append(df)
                    logger.info(f"âœ… {market_name}: {len(df)} listings")
                else:
                    logger.info(f"â„¹ï¸ {market_name}: No listings")

                # ì‹œì¥ ê°„ ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(5)

            except Exception as e:
                logger.error(f"âŒ Failed to crawl {market_name}: {e}")

        if not all_dfs:
            logger.info("No listings found in the period")
            return pl.DataFrame()

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        combined_df = pl.concat(all_dfs, how="vertical_relaxed")

        # ì¤‘ë³µ ì œê±° (ì¢…ëª©ì½”ë“œ + ìƒì¥ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •í™•í•œ ì¤‘ë³µ ì œê±°)
        if 'company_code' in combined_df.columns and 'listing_date' in combined_df.columns:
            combined_df = combined_df.unique(subset=['company_code', 'listing_date'])
        elif 'company_code' in combined_df.columns:
            combined_df = combined_df.unique(subset=['company_code'])

        logger.info(f"ğŸ¯ Total listings found: {len(combined_df)}")
        return combined_df

    def crawl_recent_listings(self, days: int = 30, listing_type: str = 'NEW') -> pl.DataFrame:
        """ìµœê·¼ Nì¼ê°„ ì‹ ê·œìƒì¥ ì¢…ëª© í¬ë¡¤ë§ (ë ˆê±°ì‹œ ë©”ì„œë“œ - í…ŒìŠ¤íŠ¸ìš©)"""
        logger.warning("Using legacy crawl_recent_listings - consider using crawl_all_listings_full_sync for production")

        end_date = datetime.now().strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=days)).strftime("%Y%m%d")

        logger.info(f"ğŸš€ Crawling recent {days} days new listings ({start_date} ~ {end_date})")

        all_dfs = []

        # KOSPI, KOSDAQ, KONEX ê°ê° í¬ë¡¤ë§
        for market_name in ['KOSPI', 'KOSDAQ', 'KONEX']:
            try:
                df = self.crawl_market(market_name, listing_type, start_date, end_date)
                if df is not None and not df.is_empty():
                    all_dfs.append(df)
                    logger.info(f"âœ… {market_name}: {len(df)} new listings")
                else:
                    logger.info(f"â„¹ï¸ {market_name}: No new listings")

                # ì‹œì¥ ê°„ ìš”ì²­ ê°„ê²©
                time.sleep(3)

            except Exception as e:
                logger.error(f"âŒ Failed to crawl {market_name}: {e}")

        if not all_dfs:
            logger.info("No new listings found in recent period")
            return pl.DataFrame()

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        combined_df = pl.concat(all_dfs, how="vertical_relaxed")

        # ì¤‘ë³µ ì œê±° (ì¢…ëª©ì½”ë“œ ê¸°ì¤€)
        if 'company_code' in combined_df.columns:
            combined_df = combined_df.unique(subset=['company_code'])

        logger.info(f"ğŸ¯ Total new listings found: {len(combined_df)}")
        return combined_df

    def crawl_all_markets_historical(self, start_date: str = "20200101", end_date: str = None) -> pl.DataFrame:
        """ëª¨ë“  ì‹œì¥ì˜ ê³¼ê±° ìƒì¥ ì´ë ¥ í¬ë¡¤ë§"""
        if end_date is None:
            end_date = datetime.now().strftime("%Y%m%d")

        logger.info(f"ğŸš€ Crawling historical listings ({start_date} ~ {end_date})")

        all_dfs = []

        for market_name in ['KOSPI', 'KOSDAQ', 'KONEX']:
            try:
                df = self.crawl_market(market_name, 'ALL', start_date, end_date)
                if df is not None and not df.is_empty():
                    all_dfs.append(df)
                    logger.info(f"âœ… {market_name}: {len(df)} listings")
                else:
                    logger.warning(f"âš ï¸ {market_name}: No data")

                # ì‹œì¥ ê°„ ìš”ì²­ ê°„ê²©
                time.sleep(3)

            except Exception as e:
                logger.error(f"âŒ Failed to crawl {market_name}: {e}")

        if not all_dfs:
            logger.warning("No listing data crawled from any market")
            return pl.DataFrame()

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        combined_df = pl.concat(all_dfs, how="vertical_relaxed")

        # ì¤‘ë³µ ì œê±° (ì¢…ëª©ì½”ë“œ + ìƒì¥ì¼ ê¸°ì¤€)
        if 'company_code' in combined_df.columns and 'listing_date' in combined_df.columns:
            combined_df = combined_df.unique(subset=['company_code', 'listing_date'])

        logger.info(f"ğŸ¯ Total historical listings: {len(combined_df)}")
        return combined_df

    def save_to_parquet(self, df: pl.DataFrame, filename: str = None) -> Path:
        """DataFrameì„ Parquet íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"new_listings_crawled_{timestamp}.parquet"

        output_path = self.data_dir / filename
        df.write_parquet(output_path)

        logger.info(f"ğŸ’¾ Data saved to: {output_path}")
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ìµœê·¼ 30ì¼ ì‹ ê·œìƒì¥ í¬ë¡¤ë§"""
    crawler = KRXNewListingCrawler()

    try:
        # ìµœê·¼ 30ì¼ ì‹ ê·œìƒì¥ í¬ë¡¤ë§
        df = crawler.crawl_recent_listings(days=30)

        if not df.is_empty():
            # Parquetìœ¼ë¡œ ì €ì¥
            output_path = crawler.save_to_parquet(df)

            # ê²°ê³¼ ìš”ì•½
            logger.info("ğŸ“Š New Listing Crawling Summary:")
            logger.info(f"  Total new listings: {len(df)}")

            if 'market' in df.columns:
                market_counts = df.group_by('market').agg(pl.count().alias('count'))
                for row in market_counts.iter_rows(named=True):
                    logger.info(f"  {row['market']}: {row['count']} listings")

            if 'listing_date' in df.columns:
                logger.info(f"  Date range: {df['listing_date'].min()} ~ {df['listing_date'].max()}")

            return True
        else:
            logger.info("No new listings found in recent period")
            return True

    except Exception as e:
        logger.error(f"âŒ New listing crawling failed: {e}")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)