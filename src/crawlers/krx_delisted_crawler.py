#!/usr/bin/env python3
"""
KRX ìƒì¥íì§€ ì¢…ëª© í¬ë¡¤ëŸ¬
https://kind.krx.co.kr/investwarn/delcompany.do?method=searchDelCompanyMain
"""

import requests
import polars as pl
from bs4 import BeautifulSoup
import logging
import time
from datetime import datetime, date
from pathlib import Path
from typing import Dict, List, Optional
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class KRXDelistedCrawler:
    """KRX ìƒì¥íì§€ ì¢…ëª© í¬ë¡¤ëŸ¬"""

    def __init__(self, data_dir: str = "data/raw"):
        self.base_url = "https://kind.krx.co.kr/investwarn/delcompany.do"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # ì„¸ì…˜ ìƒì„±
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

        # ì‹œì¥ êµ¬ë¶„ ì½”ë“œ ë§¤í•‘
        self.market_codes = {
            'KOSPI': 'Y',      # ìœ ê°€ì¦ê¶Œì‹œì¥
            'KOSDAQ': 'K',     # ì½”ìŠ¤ë‹¥ì‹œì¥
            'KONEX': 'N'       # ì½”ë„¥ìŠ¤ì‹œì¥
        }

    def _get_search_form_data(self, market_code: str, start_date: str = "19900101", end_date: str = None) -> Dict[str, str]:
        """ê²€ìƒ‰ í¼ ë°ì´í„° ìƒì„±"""
        if end_date is None:
            end_date = datetime.now().strftime("%Y%m%d")

        return {
            'method': 'searchDelCompanyList',
            'currentPageSize': '5000',  # í° í˜ì´ì§€ ì‚¬ì´ì¦ˆë¡œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            'pageIndex': '1',
            'orderMode': '0',
            'orderStat': 'D',
            'market': market_code,
            'fromDate': start_date,
            'toDate': end_date,
            'delGubun': '',  # ì „ì²´
            'companyNm': '',  # íšŒì‚¬ëª… ë¯¸ì§€ì •
        }

    def _download_excel_data(self, market_code: str, market_name: str, start_date: str = "19900101", end_date: str = None) -> Optional[Path]:
        """Excel í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì‹¤ì œë¡œëŠ” HTML íŒŒì¼)"""
        try:
            # 1. ë¨¼ì € ë©”ì¸ í˜ì´ì§€ ì ‘ì†ìœ¼ë¡œ ì„¸ì…˜ ì„¤ì •
            logger.info(f"Accessing main page for {market_name}...")
            main_response = self.session.get(f"{self.base_url}?method=searchDelCompanyMain")
            main_response.raise_for_status()

            # ì ì‹œ ëŒ€ê¸°
            time.sleep(1)

            # 2. ê²€ìƒ‰ í¼ ë°ì´í„° ì¤€ë¹„
            form_data = self._get_search_form_data(market_code, start_date, end_date)

            # 3. ê²€ìƒ‰ ì‹¤í–‰
            logger.info(f"Executing search for {market_name} market...")
            search_response = self.session.post(self.base_url, data=form_data)
            search_response.raise_for_status()

            # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            search_content = search_response.text
            if "ì˜¤ë¥˜" in search_content or "error" in search_content.lower():
                logger.warning(f"Search returned error page for {market_name}")
                # ì—ëŸ¬ê°€ ìˆì–´ë„ ê³„ì† ì‹œë„

            time.sleep(2)

            # 4. Excel ë‹¤ìš´ë¡œë“œ (POST ë°©ì‹ìœ¼ë¡œ ì‹œë„)
            excel_data = form_data.copy()
            excel_data['method'] = 'searchDelCompanyExcel'

            logger.info(f"Downloading Excel data for {market_name}...")
            excel_response = self.session.post(self.base_url, data=excel_data)
            excel_response.raise_for_status()

            # 5. íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{market_name.lower()}_delisted_{timestamp}.html"
            file_path = self.data_dir / filename

            # ì¸ì½”ë”© ì²˜ë¦¬ (KRXëŠ” ë³´í†µ EUC-KR ì‚¬ìš©)
            content = ""
            try:
                content = excel_response.content.decode('euc-kr')
            except UnicodeDecodeError:
                try:
                    content = excel_response.content.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        content = excel_response.content.decode('cp949')
                    except UnicodeDecodeError:
                        content = excel_response.content.decode('latin1')

            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)

            logger.info(f"âœ… {market_name} data downloaded: {file_path} ({len(content):,} characters)")

            # ë‚´ìš© í™•ì¸
            if "ì˜¤ë¥˜" in content or "error" in content.lower() or len(content) < 1000:
                logger.warning(f"Downloaded content might be an error page for {market_name}")

            return file_path

        except Exception as e:
            logger.error(f"âŒ Failed to download {market_name} data: {e}")
            return None

    def parse_html_to_dataframe(self, html_file: Path, market_name: str) -> pl.DataFrame:
        """HTML íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜"""
        try:
            logger.info(f"Parsing HTML file: {html_file}")

            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()

            soup = BeautifulSoup(html_content, 'html.parser')

            # í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            if not tables:
                logger.warning(f"No tables found in {html_file}")
                return pl.DataFrame()

            # ê°€ì¥ í° í…Œì´ë¸”ì„ ë°ì´í„° í…Œì´ë¸”ë¡œ ê°€ì •
            main_table = max(tables, key=lambda t: len(t.find_all('tr')))
            rows = main_table.find_all('tr')

            if len(rows) < 2:
                logger.warning(f"Not enough rows in table: {len(rows)}")
                return pl.DataFrame()

            # í—¤ë” ì¶”ì¶œ
            header_row = rows[0]
            headers = [th.get_text().strip() for th in header_row.find_all(['th', 'td'])]
            logger.info(f"Found headers: {headers}")

            # ë°ì´í„° ì¶”ì¶œ
            data_rows = []
            for row in rows[1:]:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= len(headers):
                    row_data = [cell.get_text().strip() for cell in cells[:len(headers)]]
                    data_rows.append(row_data)

            if not data_rows:
                logger.warning(f"No data rows found in {html_file}")
                return pl.DataFrame()

            # DataFrame ìƒì„±
            df_data = {header: [] for header in headers}
            for row in data_rows:
                for i, header in enumerate(headers):
                    df_data[header].append(row[i] if i < len(row) else '')

            df = pl.DataFrame(df_data)

            # ì»¬ëŸ¼ëª… ì •ê·œí™”
            df = self._normalize_columns(df, market_name)

            logger.info(f"âœ… Parsed {len(df)} records from {html_file}")
            return df

        except Exception as e:
            logger.error(f"âŒ Failed to parse {html_file}: {e}")
            return pl.DataFrame()

    def _normalize_columns(self, df: pl.DataFrame, market_name: str) -> pl.DataFrame:
        """ì»¬ëŸ¼ëª… ì •ê·œí™” ë° ë°ì´í„° íƒ€ì… ë³€í™˜"""
        try:
            # ì¼ë°˜ì ì¸ ì»¬ëŸ¼ëª… ë§¤í•‘
            column_mapping = {
                'íšŒì‚¬ëª…': 'company_name',
                'ì¢…ëª©ì½”ë“œ': 'company_code',
                'ìƒì¥íì§€ì¼': 'delisting_date',
                'ìƒì¥íì§€ì‚¬ìœ ': 'delisting_reason',
                'ë¹„ê³ ': 'remarks',
                'ìˆœë²ˆ': 'sequence'
            }

            # ì»¬ëŸ¼ëª… ë³€ê²½
            current_columns = df.columns
            rename_dict = {}
            for old_col in current_columns:
                for kr_name, en_name in column_mapping.items():
                    if kr_name in old_col:
                        rename_dict[old_col] = en_name
                        break

            if rename_dict:
                df = df.rename(rename_dict)
                logger.info(f"Renamed columns: {rename_dict}")

            # ì‹œì¥ ì •ë³´ ì¶”ê°€
            df = df.with_columns(pl.lit(market_name).alias('market'))

            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
            if 'delisting_date' in df.columns:
                df = df.with_columns(
                    pl.col('delisting_date')
                    .str.replace_all(r'[^\d]', '')  # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì œê±°
                    .str.strptime(pl.Date, format='%Y%m%d', strict=False)
                    .alias('delisting_date')
                )

            # ì¢…ëª©ì½”ë“œ ì •ë¦¬ (6ìë¦¬ ìˆ«ìë§Œ)
            if 'company_code' in df.columns:
                df = df.with_columns(
                    pl.col('company_code')
                    .str.replace_all(r'[^\d]', '')  # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì œê±°
                    .str.slice(0, 6)  # ì²˜ìŒ 6ìë¦¬ë§Œ
                    .alias('company_code')
                )

                # 6ìë¦¬ê°€ ì•„ë‹Œ ì¢…ëª©ì½”ë“œ í•„í„°ë§
                df = df.filter(pl.col('company_code').str.len_chars() == 6)

            # ë¹ˆ ë¬¸ìì—´ì„ nullë¡œ ë³€í™˜
            for col in df.columns:
                if df[col].dtype == pl.String:
                    df = df.with_columns(
                        pl.col(col).replace('', None)
                    )

            return df

        except Exception as e:
            logger.error(f"Failed to normalize columns: {e}")
            return df

    def crawl_market(self, market_name: str, start_date: str = "19900101", end_date: str = None) -> Optional[pl.DataFrame]:
        """íŠ¹ì • ì‹œì¥ì˜ ìƒì¥íì§€ ì¢…ëª© í¬ë¡¤ë§"""
        if market_name not in self.market_codes:
            logger.error(f"Invalid market name: {market_name}. Valid options: {list(self.market_codes.keys())}")
            return None

        market_code = self.market_codes[market_name]

        logger.info(f"ğŸš€ Starting crawl for {market_name} market (code: {market_code})")

        # 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        html_file = self._download_excel_data(market_code, market_name, start_date, end_date)
        if not html_file:
            return None

        # ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(2)

        # 2. HTML íŒŒì‹±
        df = self.parse_html_to_dataframe(html_file, market_name)

        return df

    def crawl_all_markets(self, start_date: str = "19900101", end_date: str = None) -> pl.DataFrame:
        """ëª¨ë“  ì‹œì¥ì˜ ìƒì¥íì§€ ì¢…ëª© í¬ë¡¤ë§"""
        logger.info("ğŸš€ Starting crawl for all markets...")

        all_dfs = []

        for market_name in self.market_codes.keys():
            try:
                df = self.crawl_market(market_name, start_date, end_date)
                if df is not None and not df.is_empty():
                    all_dfs.append(df)
                    logger.info(f"âœ… {market_name}: {len(df)} records")
                else:
                    logger.warning(f"âš ï¸ {market_name}: No data")

                # ì‹œì¥ ê°„ ìš”ì²­ ê°„ê²©
                time.sleep(3)

            except Exception as e:
                logger.error(f"âŒ Failed to crawl {market_name}: {e}")

        if not all_dfs:
            logger.warning("No data crawled from any market")
            return pl.DataFrame()

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        combined_df = pl.concat(all_dfs, how="vertical_relaxed")

        # ì¤‘ë³µ ì œê±° (ì¢…ëª©ì½”ë“œ + ìƒì¥íì§€ì¼ ê¸°ì¤€)
        if 'company_code' in combined_df.columns and 'delisting_date' in combined_df.columns:
            combined_df = combined_df.unique(subset=['company_code', 'delisting_date'])

        logger.info(f"ğŸ¯ Total crawled records: {len(combined_df)}")
        return combined_df

    def save_to_parquet(self, df: pl.DataFrame, filename: str = None) -> Path:
        """DataFrameì„ Parquet íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"delisted_stocks_crawled_{timestamp}.parquet"

        output_path = self.data_dir / filename
        df.write_parquet(output_path)

        logger.info(f"ğŸ’¾ Data saved to: {output_path}")
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = KRXDelistedCrawler()

    try:
        # ëª¨ë“  ì‹œì¥ í¬ë¡¤ë§
        df = crawler.crawl_all_markets()

        if not df.is_empty():
            # Parquetìœ¼ë¡œ ì €ì¥
            output_path = crawler.save_to_parquet(df)

            # ê²°ê³¼ ìš”ì•½
            logger.info("ğŸ“Š Crawling Summary:")
            logger.info(f"  Total records: {len(df)}")

            if 'market' in df.columns:
                market_counts = df.group_by('market').agg(pl.count().alias('count'))
                for row in market_counts.iter_rows(named=True):
                    logger.info(f"  {row['market']}: {row['count']} records")

            return True
        else:
            logger.warning("No data was crawled")
            return False

    except Exception as e:
        logger.error(f"âŒ Crawling failed: {e}")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)