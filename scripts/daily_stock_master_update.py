#!/usr/bin/env python3
"""
ì¼ê°„ ì¢…ëª© ë§ˆìŠ¤í„° ë°ì´í„° ì—…ë°ì´íŠ¸ ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸
- ìƒì¥ ì¢…ëª© ì—…ë°ì´íŠ¸ (FinanceDataReader)
- ìƒì¥íì§€ ì¢…ëª© í¬ë¡¤ë§ ë° ì—…ë°ì´íŠ¸ (KRX)
- ClickHouse ë°ì´í„° ë™ê¸°í™”
"""

import logging
import sys
from pathlib import Path
from datetime import datetime, date
from typing import Dict
import polars as pl

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.clickhouse.stock_master import ClickHouseStockMaster
from src.crawlers.krx_delisted_crawler import KRXDelistedCrawler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'logs' / f'daily_update_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DailyStockMasterUpdater:
    """ì¼ê°„ ì¢…ëª© ë§ˆìŠ¤í„° ë°ì´í„° ì—…ë°ì´íŠ¸ ê´€ë¦¬ì"""

    def __init__(self):
        self.stock_master = ClickHouseStockMaster()
        self.krx_crawler = KRXDelistedCrawler()
        self.data_dir = project_root / "data" / "daily_batch"
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def update_listed_stocks(self) -> bool:
        """ìƒì¥ ì¢…ëª© ë°ì´í„° ì—…ë°ì´íŠ¸"""
        logger.info("ğŸ”„ Starting listed stocks update...")

        try:
            import FinanceDataReader as fdr

            all_stocks = []

            # KOSPI ìƒì¥ê¸°ì—…
            logger.info("Fetching KOSPI stocks...")
            kospi_df = fdr.StockListing('KOSPI')
            kospi_pl = pl.from_pandas(kospi_df).with_columns(pl.lit('KOSPI').alias('market'))
            all_stocks.append(kospi_pl)
            logger.info(f"âœ… KOSPI stocks loaded: {len(kospi_pl)}")

            # KOSDAQ ìƒì¥ê¸°ì—…
            logger.info("Fetching KOSDAQ stocks...")
            kosdaq_df = fdr.StockListing('KOSDAQ')
            kosdaq_pl = pl.from_pandas(kosdaq_df).with_columns(pl.lit('KOSDAQ').alias('market'))
            all_stocks.append(kosdaq_pl)
            logger.info(f"âœ… KOSDAQ stocks loaded: {len(kosdaq_pl)}")

            # KONEX ìƒì¥ê¸°ì—…
            try:
                logger.info("Fetching KONEX stocks...")
                konex_df = fdr.StockListing('KONEX')
                konex_pl = pl.from_pandas(konex_df).with_columns(pl.lit('KONEX').alias('market'))
                all_stocks.append(konex_pl)
                logger.info(f"âœ… KONEX stocks loaded: {len(konex_pl)}")
            except Exception as e:
                logger.warning(f"KONEX data not available: {e}")

            # ë°ì´í„° í†µí•© ë° ì •ë¦¬
            combined_df = pl.concat(all_stocks)

            # ì»¬ëŸ¼ëª… í†µì¼
            column_mapping = {'Code': 'symbol', 'Name': 'name'}
            for old_col, new_col in column_mapping.items():
                if old_col in combined_df.columns:
                    combined_df = combined_df.rename({old_col: new_col})

            # ClickHouse ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
            cleaned_df = combined_df.select([
                pl.col('symbol').cast(pl.String).str.strip_chars(),
                pl.col('name').cast(pl.String).str.strip_chars(),
                pl.col('market').cast(pl.String),
                pl.lit(None).alias('listing_date').cast(pl.Date),
                pl.lit(None).alias('delisting_date').cast(pl.Date),
                pl.lit(1).alias('is_active').cast(pl.UInt8)
            ])

            # 6ìë¦¬ ìˆ«ì ì¢…ëª©ì½”ë“œë§Œ í•„í„°ë§
            cleaned_df = cleaned_df.filter(
                (pl.col('symbol').str.len_chars() == 6) &
                (pl.col('symbol').str.contains(r'^\d{6}$'))
            )

            # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ì´ë¦„ ì œê±°
            cleaned_df = cleaned_df.unique(subset=['symbol']).filter(pl.col('name').str.len_chars() > 0)

            logger.info(f"ğŸ“Š Processed {len(cleaned_df)} valid listed stocks")

            # ClickHouseì— ì—…ë°ì´íŠ¸ (UPSERT ë°©ì‹)
            updated_count = self._upsert_stocks(cleaned_df, is_active=True)

            logger.info(f"âœ… Listed stocks update completed: {updated_count} stocks processed")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to update listed stocks: {e}")
            return False

    def update_delisted_stocks(self) -> bool:
        """ìƒì¥íì§€ ì¢…ëª© ë°ì´í„° ì—…ë°ì´íŠ¸"""
        logger.info("ğŸ”„ Starting delisted stocks update...")

        try:
            # KRXì—ì„œ ìƒì¥íì§€ ë°ì´í„° í¬ë¡¤ë§
            delisted_df = self.krx_crawler.crawl_all_markets()

            if delisted_df.is_empty():
                logger.warning("No delisted data crawled from KRX")
                return False

            # Parquetìœ¼ë¡œ ë°±ì—… ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.data_dir / f"delisted_backup_{timestamp}.parquet"
            delisted_df.write_parquet(backup_path)
            logger.info(f"ğŸ’¾ Delisted data backed up to: {backup_path}")

            # ClickHouse ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
            processed_df = self._process_delisted_data(delisted_df)

            # ClickHouseì— ì—…ë°ì´íŠ¸
            updated_count = self._upsert_stocks(processed_df, is_active=False)

            logger.info(f"âœ… Delisted stocks update completed: {updated_count} stocks processed")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to update delisted stocks: {e}")
            return False

    def _process_delisted_data(self, df: pl.DataFrame) -> pl.DataFrame:
        """ìƒì¥íì§€ ë°ì´í„°ë¥¼ ClickHouse ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì²˜ë¦¬"""
        try:
            # ì»¬ëŸ¼ëª… ì •ê·œí™”
            column_mapping = {
                'company_code': 'symbol',
                'company_name': 'name',
                'delisting_date': 'delisting_date',
                'market': 'market'
            }

            processed_df = df
            for old_col, new_col in column_mapping.items():
                if old_col in df.columns:
                    processed_df = processed_df.rename({old_col: new_col})

            # í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ê°€ ë° ë³€í™˜
            processed_df = processed_df.with_columns([
                pl.col('symbol').cast(pl.String).str.replace_all(r'[^\d]', '').str.slice(0, 6).alias('symbol'),
                pl.when(pl.col('name').is_null()).then(pl.lit("Unknown")).otherwise(pl.col('name')).alias('name'),
                pl.when(pl.col('market').is_null()).then(pl.lit("UNKNOWN")).otherwise(pl.col('market')).alias('market'),
                pl.lit(None).alias('listing_date').cast(pl.Date),
                pl.when(pl.col('delisting_date').is_null()).then(pl.date(2020, 1, 1)).otherwise(pl.col('delisting_date')).alias('delisting_date'),
                pl.lit(0).alias('is_active').cast(pl.UInt8)
            ])

            # 6ìë¦¬ ì¢…ëª©ì½”ë“œë§Œ í•„í„°ë§
            processed_df = processed_df.filter(pl.col('symbol').str.len_chars() == 6)

            # ìµœì¢… ìŠ¤í‚¤ë§ˆ ì„ íƒ
            final_df = processed_df.select([
                'symbol', 'name', 'market', 'listing_date', 'delisting_date', 'is_active'
            ])

            return final_df

        except Exception as e:
            logger.error(f"Failed to process delisted data: {e}")
            return pl.DataFrame()

    def _upsert_stocks(self, df: pl.DataFrame, is_active: bool = True) -> int:
        """UPSERT ë°©ì‹ìœ¼ë¡œ ì¢…ëª© ë°ì´í„° ì—…ë°ì´íŠ¸"""
        if df.is_empty():
            return 0

        try:
            # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
            existing_symbols = set()
            try:
                if is_active:
                    existing_df = self.stock_master.get_active_stocks()
                else:
                    existing_df = self.stock_master.get_delisted_stocks()

                if not existing_df.is_empty():
                    existing_symbols = set(existing_df['symbol'].to_list())

            except Exception as e:
                logger.warning(f"Could not retrieve existing data: {e}")

            # ìƒˆë¡œìš´ ë°ì´í„°ë§Œ í•„í„°ë§
            if existing_symbols:
                new_df = df.filter(~pl.col('symbol').is_in(list(existing_symbols)))
                logger.info(f"Found {len(new_df)} new stocks out of {len(df)} total")
            else:
                new_df = df

            if new_df.is_empty():
                logger.info("No new stocks to insert")
                return 0

            # ë°°ì¹˜ ì‚½ì…
            return self.stock_master.insert_stocks(new_df)

        except Exception as e:
            logger.error(f"Failed to upsert stocks: {e}")
            return 0

    def optimize_and_report(self) -> Dict[str, int]:
        """í…Œì´ë¸” ìµœì í™” ë° ìµœì¢… ë¦¬í¬íŠ¸"""
        logger.info("ğŸ”§ Optimizing ClickHouse table...")

        try:
            # í…Œì´ë¸” ìµœì í™”
            self.stock_master.optimize_table()

            # ìµœì¢… í†µê³„
            stats = self.stock_master.get_stock_count()

            logger.info("ğŸ“Š Final Database Statistics:")
            total_active = 0
            total_delisted = 0

            for market, counts in stats.items():
                active = counts['active']
                delisted = counts['delisted']
                total = counts['total']

                total_active += active
                total_delisted += delisted

                logger.info(f"  {market:>10}: {active:>5} active, {delisted:>5} delisted, {total:>5} total")

            logger.info(f"  {'TOTAL':>10}: {total_active:>5} active, {total_delisted:>5} delisted, {total_active + total_delisted:>5} total")

            return {
                'total_active': total_active,
                'total_delisted': total_delisted,
                'total_stocks': total_active + total_delisted
            }

        except Exception as e:
            logger.error(f"Failed to optimize and report: {e}")
            return {}

    def run_daily_update(self) -> bool:
        """ì¼ê°„ ì „ì²´ ì—…ë°ì´íŠ¸ ì‹¤í–‰"""
        start_time = datetime.now()
        logger.info(f"ğŸš€ Starting daily stock master update at {start_time}")

        try:
            # 1. ìƒì¥ ì¢…ëª© ì—…ë°ì´íŠ¸
            listed_success = self.update_listed_stocks()

            # 2. ìƒì¥íì§€ ì¢…ëª© ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
            delisted_success = True  # KRX í¬ë¡¤ë§ì´ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ì¼ë‹¨ skip
            logger.info("âš ï¸ Skipping delisted stocks crawling due to KRX access limitations")

            # 3. ìµœì í™” ë° ë¦¬í¬íŠ¸
            final_stats = self.optimize_and_report()

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # ìµœì¢… ê²°ê³¼
            success = listed_success  # delisted_successëŠ” ì˜µì…”ë„

            if success:
                logger.info(f"âœ… Daily update completed successfully in {duration:.1f} seconds")
                logger.info(f"ğŸ“ˆ Final count: {final_stats.get('total_stocks', 0)} total stocks")
            else:
                logger.error(f"âŒ Daily update failed after {duration:.1f} seconds")

            return success

        except Exception as e:
            logger.error(f"âŒ Daily update crashed: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = project_root / 'logs'
    log_dir.mkdir(exist_ok=True)

    try:
        updater = DailyStockMasterUpdater()
        success = updater.run_daily_update()
        return success

    except Exception as e:
        logger.error(f"Failed to run daily update: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)