# Project Progress

## Current Status

**Market Data Pipeline Development**

### Completed
- âœ… PostgreSQL Docker setup (docker-compose.yml)
- âœ… ClickHouse Docker configuration added
- âœ… Basic project structure established
- âœ… Python project configuration (pyproject.toml)
- âœ… Database connection module created (src/database/connection.py)

### In Progress
- ðŸ”„ ClickHouse Docker service testing (port conflict needs resolution)

### Next Steps
- ðŸ“‹ Resolve ClickHouse port conflict with existing container
- ðŸ“‹ Test ClickHouse connectivity
- ðŸ“‹ Implement stock master data loading script
- ðŸ“‹ Set up daily batch pipeline architecture
- ðŸ“‹ Implement TDD approach for new features

## Technical Decisions

### Infrastructure
- **Database**: PostgreSQL + ClickHouse (dual database approach)
  - PostgreSQL: Metadata, configuration, OLTP operations
  - ClickHouse: Time-series market data, OLAP queries
- **Containerization**: Docker Compose
- **Python Environment**: uv (as per CLAUDE.md requirements)
- **Data Processing**: Polars (as per CLAUDE.md requirements)

### Development Rules
- Always use `uv run python` for script execution
- Always use `polars` for data processing (no pandas)
- Follow TDD: Red-Green-Refactor cycle
- Use pytest for testing

### Git Workflow Rules (NEW)
- âœ… Work on feature branches only (never directly on main)
- âœ… Auto-create branches for each feature/task
- âœ… Auto-commit ALL code changes to Git
- âœ… Auto-push to remote repository for change history tracking
- âœ… Use descriptive commit messages (conventional commit format)
- âœ… Maintain clean Git history with proper branching

## Current Challenges
1. Port 8123 already in use by existing ClickHouse container
2. Need to coordinate with existing ClickHouse setup

## Architecture Overview
```
Market Data Pipeline
â”œâ”€â”€ Data Collection Layer
â”œâ”€â”€ Storage Layer (PostgreSQL + ClickHouse)
â”œâ”€â”€ Processing Layer (Daily Batch Jobs)
â””â”€â”€ API/Interface Layer
```